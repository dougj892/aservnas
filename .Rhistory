} else if (state_grade ==5) {
states$score <- states$std5_divis_all
}
} else if (subject == "reading") {
dists$score <- dists$std35_std1_and_up
if (state_grade ==3) {
states$score <- states$std3_read_std1_all
} else if (state_grade == 5) {
states$score <- states$std5_read_std2_all
}
} else {
stop()
}
### IMPORTANT -- THIS IS WHERE I SPECIFY DISTRICT SAMPLING VARIANCE ###
var_dist_sampling <- .0016
# reshape to long
states_long <- states %>%
select(state_abbr, year, starts_with("std")) %>%
pivot_longer(cols = starts_with("std"), names_to = "series", values_to = "pct") %>%
mutate(grade = str_extract(series, "\\d"),
sector = str_extract(series, "(govt)|(pvt)|(all)"),
subject = str_extract(series, "(read)|(subtract)|(divis)")) %>%
mutate(subject = if_else(subject != "read", "math", "read"))
df_read <- states_long %>% filter((sector == "all") & (subject == "read"))
df_math <- states_long %>% filter((sector == "all") & (subject == "math"))
ggplot(df_read, aes(x = year, y = pct, color = grade)) + geom_line() +
theme(axis.text.x = element_blank())+
facet_wrap( ~ state_abbr, nrow = 3) +
labs(title = "Grade 3 and 5 reading levels")
ggsave("aser reading over time.png", path = output)
ggplot(df_math, aes(x = year, y = pct, color = grade)) + geom_line() +
theme(axis.text.x = element_blank())+
facet_wrap( ~ state_abbr, nrow = 3) +
labs(title = "Grade 3 and 5 math levels")
ggsave("aser math over time.png", path = output)
lags_read <- states %>%
group_by(state_abbr) %>%
mutate(read3_lag1 = lag(std3_read_std1_all, n = 1, order_by = year),
read3_lag2 = lag(std3_read_std1_all, n = 2, order_by = year),
read3_lag3 = lag(std3_read_std1_all, n = 3, order_by = year),
read3_lead1 = lead(std3_read_std1_all, n = 1, order_by = year),
read5_lag1 = lag(std5_read_std2_all, n = 1, order_by = year),
persist = (read3_lag1+read3_lead1)/2-read3_lag3,
delta_read3_lag2 = read3_lag2-read3_lag3,
delta_read3 = std3_read_std1_all-read3_lag1,
delta_read3_lag1 = read3_lag1 - read3_lag2,
delta_read5 = std5_read_std2_all-read5_lag1)
lags_math <- states %>%
select(year, state_abbr, ends_with("all")) %>%
group_by(state_abbr) %>%
mutate(math3_lag1 = lag(std3_subtract_all, n = 1, order_by = year),
math3_lag2 = lag(std3_subtract_all, n = 2, order_by = year),
math3_lag3 = lag(std3_subtract_all, n = 3, order_by = year),
math3_lead1 = lead(std3_subtract_all, n = 1, order_by = year),
math5_lag1 = lag(std5_divis_all, n = 1, order_by = year),
delta_math3 = std3_subtract_all-math3_lag1,
delta_math3_lag1 = math3_lag1 - math3_lag2,
delta_math3_lag2 = math3_lag2 - math3_lag3,
delta_math5 =std5_divis_all-math5_lag1)
library(broom)
lags_read_nnas <- lags_read %>% filter(!is.na(read3_lag2) & !is.na(read3_lag1))
read_resids <- lm(read3_lag2 ~ read3_lag1, lags_read_nnas)
summary(read_resids)
lags_read_nnas$resids <- residuals(read_resids)
read_resid_fit <- lm(std5_read_std2_all  ~ resids, lags_read_nnas)
summary(read_resid_fit)
lags_math_nnas <- lags_math %>% filter(!is.na(math3_lag2) & !is.na(math3_lag1))
math_resids <- lm(math3_lag2 ~ math3_lag1, lags_math_nnas)
summary(math_resids)
lags_math_nnas$resids <- residuals(math_resids)
math_resid_fit <- lm(std5_divis_all ~ resids, lags_math_nnas)
summary(math_resid_fit)
# yet another try
fit_blah <- lm(delta_read5 ~ delta_read3 + delta_read3_lag2, lags_read_nnas)
summary(fit_blah)
fit_blah_math <- lm(delta_math5 ~ delta_math3+ delta_math3_lag2, lags_math_nnas)
summary(fit_blah_math)
# Create variable for deltas and first and second lag of the delta
dist_deltas <- dists %>%
group_by(State, District) %>%
mutate(delta = score - lag(score, order_by = year)) %>%
mutate(delta_lagged = lag(delta, order_by = year), delta_dbl_lag = lag(delta, n= 2, order_by = year)) %>%
arrange(State, District, year)
# Calculate autocorrelation of delta by district and then average over all districts
# Note that since the data is already grouped by State and district, we are calculating
# autocorrelation for each district and then averaging across all districts
corr_dist_deltas <- dist_deltas %>%
filter(year >= 2008) %>%
summarise(auto = cor(delta, delta_lagged)) %>%
ungroup() %>%
summarise(mean_auto=mean(auto)) %>% .$mean_auto
# Calculate the autocorrelation in one go for all pairs of delta and delta_lagged.
# Note that we are not doing any averaging here.
# This should spit out a very similar value and it does.
check_corr_dist_deltas <- dist_deltas %>%
ungroup() %>%
filter(year >= 2008) %>%
summarise(auto = cor(delta, delta_lagged))
# Calculate the overall variance of the deltas
# Note that we have to divide by 100 here to get the actual number.
# We didn't need to do this above because we just calculated correlation and thus scale didn't matter
var_dist_deltas <- var(dist_deltas$delta/100, na.rm = TRUE)
# Calculate overall variance of the district values.
# Note that we don't want to first calculate variance of dist reading scores by state and then average
# That would really underestimate the variance of the district scores.
# (To understand intuition, note districts are considered iid in the model above.)
var_dist_levels <- var(dists$score/100, na.rm = TRUE)
# Calculate variance of the transitory component
var_dist_epsilon <- -corr_dist_deltas*var_dist_deltas
#
corr_dist_dbl_lag <- dist_deltas %>% filter(year >= 2009) %>%
ungroup() %>%
summarize(auto =cor(delta,delta_dbl_lag, use = "pairwise.complete.obs")) %>%
.$auto
# Create variable for year on year change for each district
state_deltas <- states %>%
group_by(State) %>%
mutate(delta = score - lag(score, order_by = year)) %>%
mutate(delta_lagged = lag(delta, order_by = year), delta_dbl_lag = lag(delta,n=2, order_by = year)) %>%
arrange(State, year)
# Calculate average autocorrelation of delta
corr_state_deltas <- state_deltas %>%
filter(year >= 2008) %>%
summarise(auto = cor(delta, delta_lagged, use = "pairwise.complete.obs")) %>%
ungroup() %>%
summarise(mean_auto=mean(auto)) %>% .$mean_auto
# Calculate correlation betwen delta letters and delta letters lagged for entire dataset (without first calculating for each state and then averaging).
# this should be about the same as when we first calculate for each state and then average --> which it is.
check_corr_state_deltas <- state_deltas %>% filter(year >= 2008) %>%
ungroup() %>%
summarize(auto =cor(delta,delta_lagged, use = "pairwise.complete.obs")) %>%
.$auto
# calculate the variance of the deltas
var_state_deltas <- var(state_deltas$delta, na.rm = TRUE)
# calculate the variance of the levels
var_state_levels <- var(state_deltas$score, na.rm = TRUE)
# calculate variance of transitory component
var_state_epsilon <- -corr_state_deltas*var_state_deltas
var_state_sampling <- dists %>% filter(year ==2008) %>%
count(State) %>%
mutate(sampling_var = var_dist_sampling/n) %>%
summarize(mean_sampling = mean(sampling_var)) %>% .$mean_sampling
corr_state_dbl_lag <- state_deltas %>% filter(year >= 2009) %>%
ungroup() %>%
summarize(auto =cor(delta,delta_dbl_lag, use = "pairwise.complete.obs")) %>%
.$auto
temp <- state_deltas %>% filter(year >= 2009) %>%
group_by(State) %>%
summarize(auto =cor(delta,delta_dbl_lag, use = "pairwise.complete.obs"))
# Save the value fo the correlation with double lags at both state and district level for use in
temp_df_dbl <- tibble(level = c("state", "district"), values = c(corr_state_dbl_lag, corr_dist_dbl_lag))
temp_df_dbl$subject <- subject
temp_df_dbl$state_grade <- state_grade
write_csv(temp_df_dbl, file.path(output, "intermediate", paste(subject, "-", state_grade, "- corr dbl lag.csv")))
# Generate estimates for the
change_pct_pers <- 1+2*corr_state_deltas
change_pct_samp <- 2*var_state_sampling/var_state_deltas
change_pct_other <- 1-change_pct_pers-change_pct_samp
level_pct_pers <- 1+corr_state_deltas*var_state_deltas/var_state_levels
level_pct_samp <- var_state_sampling/var_state_levels
level_pct_other <- 1-level_pct_pers-level_pct_samp
# save consolidated method 1 estimates as a csv
meth1_df <- tribble(
~changes_levels, ~grade, ~subject, ~share_persist, ~share_sampling, ~share_other,
"changes", state_grade, subject, change_pct_pers, change_pct_samp, change_pct_other,
"levels", state_grade, subject, level_pct_pers, level_pct_samp, level_pct_other,
)
write_csv(meth1_df, file.path(output, "intermediate", paste(subject, state_grade, " alt ests.csv")))
state_lags <- states %>%
group_by(State) %>%
mutate(read_lag1 = lag(score, order_by = year),
read_lag2 = lag(score, n= 2, order_by = year),
read_lag3 = lag(score, n =3, order_by = year),
read_lag4 = lag(score, n =4, order_by = year),
read_lag5 = lag(score, n =5, order_by = year)) %>%
ungroup()
rho = c()
# for each of the lags, calculate the correlation between current and the lag and store in vector rho
for (lag in seq(1,5)) {
rho <- c(rho, cor(state_lags$score, state_lags[[paste("read_lag",as.character(lag),sep="")]], use = "pairwise.complete.obs"))
}
# Save the values of rho for use in the working paper
temp_df <- tibble(rho = rho)
temp_df$subject <- subject
temp_df$state_grade <- state_grade
temp_df$lag <- 1:5
write_csv(temp_df, file.path(output, "intermediate", paste(subject, "-", state_grade, "- rho.csv")))
# Calculate variance of the transitory component by comparing rho_1 with the decay of rho after that
var_state_levels_persistent2 <- rho[1]/mean(rho/lag(rho), na.rm = TRUE)*var_state_levels
# Calculate the share of variance coming from other transitory sources.
# We will need this to calculate the variance breakup for changes
var_state_levels_other2 <- var_state_levels - var_state_levels_persistent2 - var_state_sampling
# Save a graph showing the decay of the correlation
quick_df <- tibble(lag = seq(0,5), correlation = c(1,rho))
ggplot(quick_df, aes(x= lag, y = correlation)) +
geom_line() +
geom_point() +
ylim(0,1)
# ggsave(paste(subject, "-", state_grade, "- correlation_decay.png"), width = 5, height = 6 , path = output)
# Create empty tibble to store results
# I
df_bar <- tibble(state_or_dist = rep(c("State","District"),each =6),
changes_or_levels =rep(rep(c("Changes","Levels"), each =3), times= 2),
bar_part = factor(rep(c("Persistent","Transitory sampling", "Transitory other"),4), levels = c("Transitory other","Transitory sampling", "Persistent")),
value = rep(.0016,12)
)
# Update the values for the district changes bar
df_bar$value[(df_bar$state_or_dist == "District") & (df_bar$changes_or_levels == "Changes") & (df_bar$bar_part == "Persistent")] <- var_dist_deltas - 2*var_dist_epsilon
df_bar$value[(df_bar$state_or_dist == "District") & (df_bar$changes_or_levels == "Changes") & (df_bar$bar_part == "Transitory sampling")] <- 2*var_dist_sampling
df_bar$value[(df_bar$state_or_dist == "District") & (df_bar$changes_or_levels == "Changes") & (df_bar$bar_part == "Transitory other")] <- 2*var_dist_epsilon - 2*var_dist_sampling
# Update the values for the district levels bar
df_bar$value[(df_bar$state_or_dist == "District") & (df_bar$changes_or_levels == "Levels") & (df_bar$bar_part == "Persistent")] <- var_dist_levels - var_dist_epsilon
df_bar$value[(df_bar$state_or_dist == "District") & (df_bar$changes_or_levels == "Levels") & (df_bar$bar_part == "Transitory sampling")] <- var_dist_sampling
df_bar$value[(df_bar$state_or_dist == "District") & (df_bar$changes_or_levels == "Levels") & (df_bar$bar_part == "Transitory other")] <- var_dist_epsilon - var_dist_sampling
### STATE DATA ###
# CHANGES
df_bar$value[(df_bar$state_or_dist == "State") & (df_bar$changes_or_levels == "Changes") & (df_bar$bar_part == "Persistent")] <- var_state_deltas-2*(var_state_sampling+var_state_levels_other2)
df_bar$value[(df_bar$state_or_dist == "State") & (df_bar$changes_or_levels == "Changes") & (df_bar$bar_part == "Transitory sampling")] <- 2*var_state_sampling
df_bar$value[(df_bar$state_or_dist == "State") & (df_bar$changes_or_levels == "Changes") & (df_bar$bar_part == "Transitory other")] <- 2*var_state_levels_other2
# LEVELS
df_bar$value[(df_bar$state_or_dist == "State") & (df_bar$changes_or_levels == "Levels") & (df_bar$bar_part == "Persistent")] <- var_state_levels_persistent2
df_bar$value[(df_bar$state_or_dist == "State") & (df_bar$changes_or_levels == "Levels") & (df_bar$bar_part == "Transitory sampling")] <- var_state_sampling
df_bar$value[(df_bar$state_or_dist == "State") & (df_bar$changes_or_levels == "Levels") & (df_bar$bar_part == "Transitory other")] <- var_state_levels_other2
# I have created this graph so that you can see it in the notebook
# The final version which combines both subjects, is created in a separate notebook
ggplot(df_bar, aes(fill = bar_part, y=value, x= changes_or_levels)) +
geom_bar(position="stack", stat ="identity") +
facet_grid(~ state_or_dist) +
scale_fill_manual(values = c("red", "orange", "blue"))+
labs(fill = "Variance component", x = "")
# ggsave(paste(subject, "-", state_grade," - variance_decomposition.png"), width = 5, height = 6 , path = output)
df_bar$subject <- subject
df_bar$state_grade <- state_grade
write_csv(df_bar, file.path(output, "intermediate", paste(subject, "-", state_grade, "- bar data.csv")))
df_final <- df_bar %>%
pivot_wider(names_from = "bar_part", values_from= "value") %>%
mutate(total_var = `Persistent`+`Transitory sampling`+`Transitory other`) %>%
mutate(share_pers = `Persistent`/total_var, share_samp = `Transitory sampling`/total_var, share_other = `Transitory other`/total_var)
df_final$subject <- subject
df_final$state_grade <- state_grade
write_csv(df_final, file.path(output, "intermediate", paste(subject, "-", state_grade, "- var deco.csv")))
temp <- lm(std5_read_std2_all~ read5_lag + std3_read_std1_all)
temp <- lm(std5_read_std2_all~ read5_lag + std3_read_std1_all, lags_read)
temp <- lm(std5_read_std2_all~ read5_lag1 + std3_read_std1_all, lags_read)
summary(temp)
temp <- lm(std5_read_std2_all~ read5_lag1 + std3_read_std1_all + read3_lag2, lags_read)
summary(temp)
library(tidyverse)
output <- "C:/Users/dougj/Dropbox/Education in India/Original research/aservnas/figures"
dists <- read_csv("https://raw.githubusercontent.com/dougj892/public-datasets/master/aser_district_all_6.csv")
states <- read_csv("https://raw.githubusercontent.com/dougj892/public-datasets/master/ASER%20trends%20over%20time.csv") %>%
select(year, State, state_abbr, ends_with("all"))
##### IMPORTANT: SET SUBJECT TO BE USED FOR THE ENTIRE ANALYSIS HERE ###
subject <- "math"
state_grade <- 3
if (subject == "math") {
dists$score <- dists$std35_subtraction_and_up
if (state_grade ==3) {
states$score <- states$std3_subtract_all
} else if (state_grade ==5) {
states$score <- states$std5_divis_all
}
} else if (subject == "reading") {
dists$score <- dists$std35_std1_and_up
if (state_grade ==3) {
states$score <- states$std3_read_std1_all
} else if (state_grade == 5) {
states$score <- states$std5_read_std2_all
}
} else {
stop()
}
### IMPORTANT -- THIS IS WHERE I SPECIFY DISTRICT SAMPLING VARIANCE ###
var_dist_sampling <- .0016
# reshape to long
states_long <- states %>%
select(state_abbr, year, starts_with("std")) %>%
pivot_longer(cols = starts_with("std"), names_to = "series", values_to = "pct") %>%
mutate(grade = str_extract(series, "\\d"),
sector = str_extract(series, "(govt)|(pvt)|(all)"),
subject = str_extract(series, "(read)|(subtract)|(divis)")) %>%
mutate(subject = if_else(subject != "read", "math", "read"))
df_read <- states_long %>% filter((sector == "all") & (subject == "read"))
df_math <- states_long %>% filter((sector == "all") & (subject == "math"))
ggplot(df_read, aes(x = year, y = pct, color = grade)) + geom_line() +
theme(axis.text.x = element_blank())+
facet_wrap( ~ state_abbr, nrow = 3) +
labs(title = "Grade 3 and 5 reading levels")
ggsave("aser reading over time.png", path = output)
ggplot(df_math, aes(x = year, y = pct, color = grade)) + geom_line() +
theme(axis.text.x = element_blank())+
facet_wrap( ~ state_abbr, nrow = 3) +
labs(title = "Grade 3 and 5 math levels")
ggsave("aser math over time.png", path = output)
lags_read <- states %>%
group_by(state_abbr) %>%
mutate(read3_lag1 = lag(std3_read_std1_all, n = 1, order_by = year),
read3_lag2 = lag(std3_read_std1_all, n = 2, order_by = year),
read3_lag3 = lag(std3_read_std1_all, n = 3, order_by = year),
read3_lead1 = lead(std3_read_std1_all, n = 1, order_by = year),
read5_lag1 = lag(std5_read_std2_all, n = 1, order_by = year),
persist = (read3_lag1+read3_lead1)/2-read3_lag3,
delta_read3_lag2 = read3_lag2-read3_lag3,
delta_read3 = std3_read_std1_all-read3_lag1,
delta_read3_lag1 = read3_lag1 - read3_lag2,
delta_read5 = std5_read_std2_all-read5_lag1)
lags_math <- states %>%
select(year, state_abbr, ends_with("all")) %>%
group_by(state_abbr) %>%
mutate(math3_lag1 = lag(std3_subtract_all, n = 1, order_by = year),
math3_lag2 = lag(std3_subtract_all, n = 2, order_by = year),
math3_lag3 = lag(std3_subtract_all, n = 3, order_by = year),
math3_lead1 = lead(std3_subtract_all, n = 1, order_by = year),
math5_lag1 = lag(std5_divis_all, n = 1, order_by = year),
delta_math3 = std3_subtract_all-math3_lag1,
delta_math3_lag1 = math3_lag1 - math3_lag2,
delta_math3_lag2 = math3_lag2 - math3_lag3,
delta_math5 =std5_divis_all-math5_lag1)
library(broom)
lags_read_nnas <- lags_read %>% filter(!is.na(read3_lag2) & !is.na(read3_lag1))
read_resids <- lm(read3_lag2 ~ read3_lag1, lags_read_nnas)
summary(read_resids)
lags_read_nnas$resids <- residuals(read_resids)
read_resid_fit <- lm(std5_read_std2_all  ~ resids, lags_read_nnas)
summary(read_resid_fit)
lags_math_nnas <- lags_math %>% filter(!is.na(math3_lag2) & !is.na(math3_lag1))
math_resids <- lm(math3_lag2 ~ math3_lag1, lags_math_nnas)
summary(math_resids)
lags_math_nnas$resids <- residuals(math_resids)
math_resid_fit <- lm(std5_divis_all ~ resids, lags_math_nnas)
summary(math_resid_fit)
# yet another try
fit_blah <- lm(delta_read5 ~ delta_read3 + delta_read3_lag2, lags_read_nnas)
summary(fit_blah)
fit_blah_math <- lm(delta_math5 ~ delta_math3+ delta_math3_lag2, lags_math_nnas)
summary(fit_blah_math)
# Create variable for deltas and first and second lag of the delta
dist_deltas <- dists %>%
group_by(State, District) %>%
mutate(delta = score - lag(score, order_by = year)) %>%
mutate(delta_lagged = lag(delta, order_by = year), delta_dbl_lag = lag(delta, n= 2, order_by = year)) %>%
arrange(State, District, year)
# Calculate autocorrelation of delta by district and then average over all districts
# Note that since the data is already grouped by State and district, we are calculating
# autocorrelation for each district and then averaging across all districts
corr_dist_deltas <- dist_deltas %>%
filter(year >= 2008) %>%
summarise(auto = cor(delta, delta_lagged)) %>%
ungroup() %>%
summarise(mean_auto=mean(auto)) %>% .$mean_auto
# Calculate the autocorrelation in one go for all pairs of delta and delta_lagged.
# Note that we are not doing any averaging here.
# This should spit out a very similar value and it does.
check_corr_dist_deltas <- dist_deltas %>%
ungroup() %>%
filter(year >= 2008) %>%
summarise(auto = cor(delta, delta_lagged))
# Calculate the overall variance of the deltas
# Note that we have to divide by 100 here to get the actual number.
# We didn't need to do this above because we just calculated correlation and thus scale didn't matter
var_dist_deltas <- var(dist_deltas$delta/100, na.rm = TRUE)
# Calculate overall variance of the district values.
# Note that we don't want to first calculate variance of dist reading scores by state and then average
# That would really underestimate the variance of the district scores.
# (To understand intuition, note districts are considered iid in the model above.)
var_dist_levels <- var(dists$score/100, na.rm = TRUE)
# Calculate variance of the transitory component
var_dist_epsilon <- -corr_dist_deltas*var_dist_deltas
#
corr_dist_dbl_lag <- dist_deltas %>% filter(year >= 2009) %>%
ungroup() %>%
summarize(auto =cor(delta,delta_dbl_lag, use = "pairwise.complete.obs")) %>%
.$auto
# Create variable for year on year change for each district
state_deltas <- states %>%
group_by(State) %>%
mutate(delta = score - lag(score, order_by = year)) %>%
mutate(delta_lagged = lag(delta, order_by = year), delta_dbl_lag = lag(delta,n=2, order_by = year)) %>%
arrange(State, year)
# Calculate average autocorrelation of delta
corr_state_deltas <- state_deltas %>%
filter(year >= 2008) %>%
summarise(auto = cor(delta, delta_lagged, use = "pairwise.complete.obs")) %>%
ungroup() %>%
summarise(mean_auto=mean(auto)) %>% .$mean_auto
# Calculate correlation betwen delta letters and delta letters lagged for entire dataset (without first calculating for each state and then averaging).
# this should be about the same as when we first calculate for each state and then average --> which it is.
check_corr_state_deltas <- state_deltas %>% filter(year >= 2008) %>%
ungroup() %>%
summarize(auto =cor(delta,delta_lagged, use = "pairwise.complete.obs")) %>%
.$auto
# calculate the variance of the deltas
var_state_deltas <- var(state_deltas$delta, na.rm = TRUE)
# calculate the variance of the levels
var_state_levels <- var(state_deltas$score, na.rm = TRUE)
# calculate variance of transitory component
var_state_epsilon <- -corr_state_deltas*var_state_deltas
var_state_sampling <- dists %>% filter(year ==2008) %>%
count(State) %>%
mutate(sampling_var = var_dist_sampling/n) %>%
summarize(mean_sampling = mean(sampling_var)) %>% .$mean_sampling
corr_state_dbl_lag <- state_deltas %>% filter(year >= 2009) %>%
ungroup() %>%
summarize(auto =cor(delta,delta_dbl_lag, use = "pairwise.complete.obs")) %>%
.$auto
temp <- state_deltas %>% filter(year >= 2009) %>%
group_by(State) %>%
summarize(auto =cor(delta,delta_dbl_lag, use = "pairwise.complete.obs"))
# Save the value fo the correlation with double lags at both state and district level for use in
temp_df_dbl <- tibble(level = c("state", "district"), values = c(corr_state_dbl_lag, corr_dist_dbl_lag))
temp_df_dbl$subject <- subject
temp_df_dbl$state_grade <- state_grade
write_csv(temp_df_dbl, file.path(output, "intermediate", paste(subject, "-", state_grade, "- corr dbl lag.csv")))
# Generate estimates for the
change_pct_pers <- 1+2*corr_state_deltas
change_pct_samp <- 2*var_state_sampling/var_state_deltas
change_pct_other <- 1-change_pct_pers-change_pct_samp
level_pct_pers <- 1+corr_state_deltas*var_state_deltas/var_state_levels
level_pct_samp <- var_state_sampling/var_state_levels
level_pct_other <- 1-level_pct_pers-level_pct_samp
# save consolidated method 1 estimates as a csv
meth1_df <- tribble(
~changes_levels, ~grade, ~subject, ~share_persist, ~share_sampling, ~share_other,
"changes", state_grade, subject, change_pct_pers, change_pct_samp, change_pct_other,
"levels", state_grade, subject, level_pct_pers, level_pct_samp, level_pct_other,
)
write_csv(meth1_df, file.path(output, "intermediate", paste(subject, state_grade, " alt ests.csv")))
state_lags <- states %>%
group_by(State) %>%
mutate(read_lag1 = lag(score, order_by = year),
read_lag2 = lag(score, n= 2, order_by = year),
read_lag3 = lag(score, n =3, order_by = year),
read_lag4 = lag(score, n =4, order_by = year),
read_lag5 = lag(score, n =5, order_by = year)) %>%
ungroup()
rho = c()
# for each of the lags, calculate the correlation between current and the lag and store in vector rho
for (lag in seq(1,5)) {
rho <- c(rho, cor(state_lags$score, state_lags[[paste("read_lag",as.character(lag),sep="")]], use = "pairwise.complete.obs"))
}
# Save the values of rho for use in the working paper
temp_df <- tibble(rho = rho)
temp_df$subject <- subject
temp_df$state_grade <- state_grade
temp_df$lag <- 1:5
write_csv(temp_df, file.path(output, "intermediate", paste(subject, "-", state_grade, "- rho.csv")))
# Calculate variance of the transitory component by comparing rho_1 with the decay of rho after that
var_state_levels_persistent2 <- rho[1]/mean(rho/lag(rho), na.rm = TRUE)*var_state_levels
# Calculate the share of variance coming from other transitory sources.
# We will need this to calculate the variance breakup for changes
var_state_levels_other2 <- var_state_levels - var_state_levels_persistent2 - var_state_sampling
# Save a graph showing the decay of the correlation
quick_df <- tibble(lag = seq(0,5), correlation = c(1,rho))
ggplot(quick_df, aes(x= lag, y = correlation)) +
geom_line() +
geom_point() +
ylim(0,1)
# ggsave(paste(subject, "-", state_grade, "- correlation_decay.png"), width = 5, height = 6 , path = output)
# Create empty tibble to store results
# I
df_bar <- tibble(state_or_dist = rep(c("State","District"),each =6),
changes_or_levels =rep(rep(c("Changes","Levels"), each =3), times= 2),
bar_part = factor(rep(c("Persistent","Transitory sampling", "Transitory other"),4), levels = c("Transitory other","Transitory sampling", "Persistent")),
value = rep(.0016,12)
)
# Update the values for the district changes bar
df_bar$value[(df_bar$state_or_dist == "District") & (df_bar$changes_or_levels == "Changes") & (df_bar$bar_part == "Persistent")] <- var_dist_deltas - 2*var_dist_epsilon
df_bar$value[(df_bar$state_or_dist == "District") & (df_bar$changes_or_levels == "Changes") & (df_bar$bar_part == "Transitory sampling")] <- 2*var_dist_sampling
df_bar$value[(df_bar$state_or_dist == "District") & (df_bar$changes_or_levels == "Changes") & (df_bar$bar_part == "Transitory other")] <- 2*var_dist_epsilon - 2*var_dist_sampling
# Update the values for the district levels bar
df_bar$value[(df_bar$state_or_dist == "District") & (df_bar$changes_or_levels == "Levels") & (df_bar$bar_part == "Persistent")] <- var_dist_levels - var_dist_epsilon
df_bar$value[(df_bar$state_or_dist == "District") & (df_bar$changes_or_levels == "Levels") & (df_bar$bar_part == "Transitory sampling")] <- var_dist_sampling
df_bar$value[(df_bar$state_or_dist == "District") & (df_bar$changes_or_levels == "Levels") & (df_bar$bar_part == "Transitory other")] <- var_dist_epsilon - var_dist_sampling
### STATE DATA ###
# CHANGES
df_bar$value[(df_bar$state_or_dist == "State") & (df_bar$changes_or_levels == "Changes") & (df_bar$bar_part == "Persistent")] <- var_state_deltas-2*(var_state_sampling+var_state_levels_other2)
df_bar$value[(df_bar$state_or_dist == "State") & (df_bar$changes_or_levels == "Changes") & (df_bar$bar_part == "Transitory sampling")] <- 2*var_state_sampling
df_bar$value[(df_bar$state_or_dist == "State") & (df_bar$changes_or_levels == "Changes") & (df_bar$bar_part == "Transitory other")] <- 2*var_state_levels_other2
# LEVELS
df_bar$value[(df_bar$state_or_dist == "State") & (df_bar$changes_or_levels == "Levels") & (df_bar$bar_part == "Persistent")] <- var_state_levels_persistent2
df_bar$value[(df_bar$state_or_dist == "State") & (df_bar$changes_or_levels == "Levels") & (df_bar$bar_part == "Transitory sampling")] <- var_state_sampling
df_bar$value[(df_bar$state_or_dist == "State") & (df_bar$changes_or_levels == "Levels") & (df_bar$bar_part == "Transitory other")] <- var_state_levels_other2
# I have created this graph so that you can see it in the notebook
# The final version which combines both subjects, is created in a separate notebook
ggplot(df_bar, aes(fill = bar_part, y=value, x= changes_or_levels)) +
geom_bar(position="stack", stat ="identity") +
facet_grid(~ state_or_dist) +
scale_fill_manual(values = c("red", "orange", "blue"))+
labs(fill = "Variance component", x = "")
# ggsave(paste(subject, "-", state_grade," - variance_decomposition.png"), width = 5, height = 6 , path = output)
df_bar$subject <- subject
df_bar$state_grade <- state_grade
write_csv(df_bar, file.path(output, "intermediate", paste(subject, "-", state_grade, "- bar data.csv")))
df_final <- df_bar %>%
pivot_wider(names_from = "bar_part", values_from= "value") %>%
mutate(total_var = `Persistent`+`Transitory sampling`+`Transitory other`) %>%
mutate(share_pers = `Persistent`/total_var, share_samp = `Transitory sampling`/total_var, share_other = `Transitory other`/total_var)
df_final$subject <- subject
df_final$state_grade <- state_grade
write_csv(df_final, file.path(output, "intermediate", paste(subject, "-", state_grade, "- var deco.csv")))
summary(fit_blah)
summary(fit_blah_math)
# yet another try
fit_blah <- lm(delta_read5 ~  delta_read3_lag2, lags_read_nnas)
summary(fit_blah)
fit_blah_math <- lm(delta_math5 ~ delta_math3_lag2, lags_math_nnas)
summary(fit_blah_math)
# yet another try
fit_blah <- lm(delta_read5 ~  delta_read3_lag2-1, lags_read_nnas)
summary(fit_blah)
fit_blah_math <- lm(delta_math5 ~ delta_math3_lag2-1, lags_math_nnas)
summary(fit_blah_math)
