group_by(State) %>%
summarize(auto =cor(delta,delta_dbl_lag, use = "pairwise.complete.obs"))
# Save the value fo the correlation with double lags at both state and district level for use in
temp_df_dbl <- tibble(level = c("state", "district"), values = c(corr_state_dbl_lag, corr_dist_dbl_lag))
temp_df_dbl$subject <- subject
temp_df_dbl$state_grade <- state_grade
write_csv(temp_df_dbl, file.path(output, "intermediate", paste(subject, "-", state_grade, "- corr dbl lag.csv")))
# Generate estimates for the
change_pct_pers <- 1+2*corr_state_deltas
change_pct_samp <- 2*var_state_sampling/var_state_deltas
change_pct_other <- 1-change_pct_pers-change_pct_samp
level_pct_pers <- 1+corr_state_deltas*var_state_deltas/var_state_levels
level_pct_samp <- var_state_sampling/var_state_levels
level_pct_other <- 1-level_pct_pers-level_pct_samp
# save consolidated method 1 estimates as a csv
meth1_df <- tribble(
~changes_levels, ~grade, ~subject, ~share_persist, ~share_sampling, ~share_other,
"changes", state_grade, subject, change_pct_pers, change_pct_samp, change_pct_other,
"levels", state_grade, subject, level_pct_pers, level_pct_samp, level_pct_other,
)
write_csv(meth1_df, file.path(output, "intermediate", paste(subject, state_grade, " alt ests.csv")))
state_lags <- states %>%
group_by(State) %>%
mutate(read_lag1 = lag(score, order_by = year),
read_lag2 = lag(score, n= 2, order_by = year),
read_lag3 = lag(score, n =3, order_by = year),
read_lag4 = lag(score, n =4, order_by = year),
read_lag5 = lag(score, n =5, order_by = year)) %>%
ungroup()
rho = c()
# for each of the lags, calculate the correlation between current and the lag and store in vector rho
for (lag in seq(1,5)) {
rho <- c(rho, cor(state_lags$score, state_lags[[paste("read_lag",as.character(lag),sep="")]], use = "pairwise.complete.obs"))
}
# Save the values of rho for use in the working paper
temp_df <- tibble(rho = rho)
temp_df$subject <- subject
temp_df$state_grade <- state_grade
temp_df$lag <- 1:5
write_csv(temp_df, file.path(output, "intermediate", paste(subject, "-", state_grade, "- rho.csv")))
# Calculate variance of the transitory component by comparing rho_1 with the decay of rho after that
var_state_levels_persistent2 <- rho[1]/mean(rho/lag(rho), na.rm = TRUE)*var_state_levels
# Calculate the share of variance coming from other transitory sources.
# We will need this to calculate the variance breakup for changes
var_state_levels_other2 <- var_state_levels - var_state_levels_persistent2 - var_state_sampling
# Save a graph showing the decay of the correlation
quick_df <- tibble(lag = seq(0,5), correlation = c(1,rho))
ggplot(quick_df, aes(x= lag, y = correlation)) +
geom_line() +
geom_point() +
ylim(0,1)
# ggsave(paste(subject, "-", state_grade, "- correlation_decay.png"), width = 5, height = 6 , path = output)
# Create empty tibble to store results
# I
df_bar <- tibble(state_or_dist = rep(c("State","District"),each =6),
changes_or_levels =rep(rep(c("Changes","Levels"), each =3), times= 2),
bar_part = factor(rep(c("Persistent","Transitory sampling", "Transitory other"),4), levels = c("Transitory other","Transitory sampling", "Persistent")),
value = rep(.001,12)
)
# Update the values for the district changes bar
df_bar$value[(df_bar$state_or_dist == "District") & (df_bar$changes_or_levels == "Changes") & (df_bar$bar_part == "Persistent")] <- var_dist_deltas - 2*var_dist_epsilon
df_bar$value[(df_bar$state_or_dist == "District") & (df_bar$changes_or_levels == "Changes") & (df_bar$bar_part == "Transitory sampling")] <- 2*var_dist_sampling
df_bar$value[(df_bar$state_or_dist == "District") & (df_bar$changes_or_levels == "Changes") & (df_bar$bar_part == "Transitory other")] <- 2*var_dist_epsilon - 2*var_dist_sampling
# Update the values for the district levels bar
df_bar$value[(df_bar$state_or_dist == "District") & (df_bar$changes_or_levels == "Levels") & (df_bar$bar_part == "Persistent")] <- var_dist_levels - var_dist_epsilon
df_bar$value[(df_bar$state_or_dist == "District") & (df_bar$changes_or_levels == "Levels") & (df_bar$bar_part == "Transitory sampling")] <- var_dist_sampling
df_bar$value[(df_bar$state_or_dist == "District") & (df_bar$changes_or_levels == "Levels") & (df_bar$bar_part == "Transitory other")] <- var_dist_epsilon - var_dist_sampling
### STATE DATA ###
# CHANGES
df_bar$value[(df_bar$state_or_dist == "State") & (df_bar$changes_or_levels == "Changes") & (df_bar$bar_part == "Persistent")] <- var_state_deltas-2*(var_state_sampling+var_state_levels_other2)
df_bar$value[(df_bar$state_or_dist == "State") & (df_bar$changes_or_levels == "Changes") & (df_bar$bar_part == "Transitory sampling")] <- 2*var_state_sampling
df_bar$value[(df_bar$state_or_dist == "State") & (df_bar$changes_or_levels == "Changes") & (df_bar$bar_part == "Transitory other")] <- 2*var_state_levels_other2
# LEVELS
df_bar$value[(df_bar$state_or_dist == "State") & (df_bar$changes_or_levels == "Levels") & (df_bar$bar_part == "Persistent")] <- var_state_levels_persistent2
df_bar$value[(df_bar$state_or_dist == "State") & (df_bar$changes_or_levels == "Levels") & (df_bar$bar_part == "Transitory sampling")] <- var_state_sampling
df_bar$value[(df_bar$state_or_dist == "State") & (df_bar$changes_or_levels == "Levels") & (df_bar$bar_part == "Transitory other")] <- var_state_levels_other2
# I have created this graph so that you can see it in the notebook
# The final version which combines both subjects, is created in a separate notebook
ggplot(df_bar, aes(fill = bar_part, y=value, x= changes_or_levels)) +
geom_bar(position="stack", stat ="identity") +
facet_grid(~ state_or_dist) +
scale_fill_manual(values = c("red", "orange", "blue"))+
labs(fill = "Variance component", x = "")
# ggsave(paste(subject, "-", state_grade," - variance_decomposition.png"), width = 5, height = 6 , path = output)
df_bar$subject <- subject
df_bar$state_grade <- state_grade
write_csv(df_bar, file.path(output, "intermediate", paste(subject, "-", state_grade, "- bar data.csv")))
df_final <- df_bar %>%
pivot_wider(names_from = "bar_part", values_from= "value") %>%
mutate(total_var = `Persistent`+`Transitory sampling`+`Transitory other`) %>%
mutate(share_pers = `Persistent`/total_var, share_samp = `Transitory sampling`/total_var, share_other = `Transitory other`/total_var)
df_final$subject <- subject
df_final$state_grade <- state_grade
write_csv(df_final, file.path(output, "intermediate", paste(subject, "-", state_grade, "- var deco.csv")))
library(tidyverse)
output <- "C:/Users/dougj/Dropbox/Education in India/Original research/aservnas/figures"
dists <- read_csv("https://raw.githubusercontent.com/dougj892/public-datasets/master/aser_district_all_6.csv")
states <- read_csv("https://raw.githubusercontent.com/dougj892/public-datasets/master/ASER%20trends%20over%20time.csv") %>%
select(year, State, state_abbr, ends_with("all"))
##### IMPORTANT: SET SUBJECT TO BE USED FOR THE ENTIRE ANALYSIS HERE ###
subject <- "reading"
state_grade <- 3
if (subject == "math") {
dists$score <- dists$std35_subtraction_and_up
if (state_grade ==3) {
states$score <- states$std3_subtract_all
} else if (state_grade ==5) {
states$score <- states$std5_divis_all
}
} else if (subject == "reading") {
dists$score <- dists$std35_std1_and_up
if (state_grade ==3) {
states$score <- states$std3_read_std1_all
} else if (state_grade == 5) {
states$score <- states$std5_read_std2_all
}
} else {
stop()
}
### IMPORTANT -- THIS IS WHERE I SPECIFY DISTRICT SAMPLING VARIANCE ###
var_dist_sampling <- .0013
# reshape to long
states_long <- states %>%
select(state_abbr, year, starts_with("std")) %>%
pivot_longer(cols = starts_with("std"), names_to = "series", values_to = "pct") %>%
mutate(grade = str_extract(series, "\\d"),
sector = str_extract(series, "(govt)|(pvt)|(all)"),
subject = str_extract(series, "(read)|(subtract)|(divis)")) %>%
mutate(subject = if_else(subject != "read", "math", "read"))
df_read <- states_long %>% filter((sector == "all") & (subject == "read"))
df_math <- states_long %>% filter((sector == "all") & (subject == "math"))
ggplot(df_read, aes(x = year, y = pct, color = grade)) + geom_line() +
theme(axis.text.x = element_blank())+
facet_wrap( ~ state_abbr, nrow = 3) +
labs(title = "Grade 3 and 5 reading levels")
ggsave("aser reading over time.png", path = output)
ggplot(df_math, aes(x = year, y = pct, color = grade)) + geom_line() +
theme(axis.text.x = element_blank())+
facet_wrap( ~ state_abbr, nrow = 3) +
labs(title = "Grade 3 and 5 math levels")
ggsave("aser math over time.png", path = output)
lags_read <- states %>%
group_by(state_abbr) %>%
mutate(read3_lag1 = lag(std3_read_std1_all, n = 1, order_by = year),
read3_lag2 = lag(std3_read_std1_all, n = 2, order_by = year),
read3_lag3 = lag(std3_read_std1_all, n = 3, order_by = year),
read3_lead1 = lead(std3_read_std1_all, n = 1, order_by = year),
read5_lag1 = lag(std5_read_std2_all, n = 1, order_by = year),
persist = (read3_lag1+read3_lead1)/2-read3_lag3,
trans = read3_lag2-read3_lag3)
lags_math <- states %>%
select(year, state_abbr, ends_with("all")) %>%
group_by(state_abbr) %>%
mutate(math3_lag1 = lag(std3_subtract_all, n = 1, order_by = year),
math3_lag2 = lag(std3_subtract_all, n = 2, order_by = year),
math3_lag3 = lag(std3_subtract_all, n = 3, order_by = year),
math3_lead1 = lead(std3_subtract_all, n = 1, order_by = year),
math5_lag1 = lag(std5_divis_all, n = 1, order_by = year),
persist = (math3_lag1+math3_lead1)/2-math3_lag3,
trans = math3_lag2-math3_lag3)
# reading regression
read_fit <- lm(std5_read_std2_all ~ read5_lag1 + std3_read_std1_all + trans, lags_read)
summary(read_fit)
# reading regression
read_fit <- lm(std5_divis_all ~ math5_lag1 + std3_subtract_all + trans, lags_math)
summary(read_fit)
# Create variable for deltas and first and second lag of the delta
dist_deltas <- dists %>%
group_by(State, District) %>%
mutate(delta = score - lag(score, order_by = year)) %>%
mutate(delta_lagged = lag(delta, order_by = year), delta_dbl_lag = lag(delta, n= 2, order_by = year)) %>%
arrange(State, District, year)
# Calculate autocorrelation of delta by district and then average over all districts
# Note that since the data is already grouped by State and district, we are calculating
# autocorrelation for each district and then averaging across all districts
corr_dist_deltas <- dist_deltas %>%
filter(year >= 2008) %>%
summarise(auto = cor(delta, delta_lagged)) %>%
ungroup() %>%
summarise(mean_auto=mean(auto)) %>% .$mean_auto
# Calculate the autocorrelation in one go for all pairs of delta and delta_lagged.
# Note that we are not doing any averaging here.
# This should spit out a very similar value and it does.
check_corr_dist_deltas <- dist_deltas %>%
ungroup() %>%
filter(year >= 2008) %>%
summarise(auto = cor(delta, delta_lagged))
# Calculate the overall variance of the deltas
# Note that we have to divide by 100 here to get the actual number.
# We didn't need to do this above because we just calculated correlation and thus scale didn't matter
var_dist_deltas <- var(dist_deltas$delta/100, na.rm = TRUE)
# Calculate overall variance of the district values.
# Note that we don't want to first calculate variance of dist reading scores by state and then average
# That would really underestimate the variance of the district scores.
# (To understand intuition, note districts are considered iid in the model above.)
var_dist_levels <- var(dists$score/100, na.rm = TRUE)
# Calculate variance of the transitory component
var_dist_epsilon <- -corr_dist_deltas*var_dist_deltas
#
corr_dist_dbl_lag <- dist_deltas %>% filter(year >= 2009) %>%
ungroup() %>%
summarize(auto =cor(delta,delta_dbl_lag, use = "pairwise.complete.obs")) %>%
.$auto
# Create variable for year on year change for each district
state_deltas <- states %>%
group_by(State) %>%
mutate(delta = score - lag(score, order_by = year)) %>%
mutate(delta_lagged = lag(delta, order_by = year), delta_dbl_lag = lag(delta,n=2, order_by = year)) %>%
arrange(State, year)
# Calculate average autocorrelation of delta
corr_state_deltas <- state_deltas %>%
filter(year >= 2008) %>%
summarise(auto = cor(delta, delta_lagged, use = "pairwise.complete.obs")) %>%
ungroup() %>%
summarise(mean_auto=mean(auto)) %>% .$mean_auto
# Calculate correlation betwen delta letters and delta letters lagged for entire dataset (without first calculating for each state and then averaging).
# this should be about the same as when we first calculate for each state and then average --> which it is.
check_corr_state_deltas <- state_deltas %>% filter(year >= 2008) %>%
ungroup() %>%
summarize(auto =cor(delta,delta_lagged, use = "pairwise.complete.obs")) %>%
.$auto
# calculate the variance of the deltas
var_state_deltas <- var(state_deltas$delta, na.rm = TRUE)
# calculate the variance of the levels
var_state_levels <- var(state_deltas$score, na.rm = TRUE)
# calculate variance of transitory component
var_state_epsilon <- -corr_state_deltas*var_state_deltas
var_state_sampling <- dists %>% filter(year ==2008) %>%
count(State) %>%
mutate(sampling_var = var_dist_sampling/n) %>%
summarize(mean_sampling = mean(sampling_var)) %>% .$mean_sampling
corr_state_dbl_lag <- state_deltas %>% filter(year >= 2009) %>%
ungroup() %>%
summarize(auto =cor(delta,delta_dbl_lag, use = "pairwise.complete.obs")) %>%
.$auto
temp <- state_deltas %>% filter(year >= 2009) %>%
group_by(State) %>%
summarize(auto =cor(delta,delta_dbl_lag, use = "pairwise.complete.obs"))
# Save the value fo the correlation with double lags at both state and district level for use in
temp_df_dbl <- tibble(level = c("state", "district"), values = c(corr_state_dbl_lag, corr_dist_dbl_lag))
temp_df_dbl$subject <- subject
temp_df_dbl$state_grade <- state_grade
write_csv(temp_df_dbl, file.path(output, "intermediate", paste(subject, "-", state_grade, "- corr dbl lag.csv")))
# Generate estimates for the
change_pct_pers <- 1+2*corr_state_deltas
change_pct_samp <- 2*var_state_sampling/var_state_deltas
change_pct_other <- 1-change_pct_pers-change_pct_samp
level_pct_pers <- 1+corr_state_deltas*var_state_deltas/var_state_levels
level_pct_samp <- var_state_sampling/var_state_levels
level_pct_other <- 1-level_pct_pers-level_pct_samp
# save consolidated method 1 estimates as a csv
meth1_df <- tribble(
~changes_levels, ~grade, ~subject, ~share_persist, ~share_sampling, ~share_other,
"changes", state_grade, subject, change_pct_pers, change_pct_samp, change_pct_other,
"levels", state_grade, subject, level_pct_pers, level_pct_samp, level_pct_other,
)
write_csv(meth1_df, file.path(output, "intermediate", paste(subject, state_grade, " alt ests.csv")))
state_lags <- states %>%
group_by(State) %>%
mutate(read_lag1 = lag(score, order_by = year),
read_lag2 = lag(score, n= 2, order_by = year),
read_lag3 = lag(score, n =3, order_by = year),
read_lag4 = lag(score, n =4, order_by = year),
read_lag5 = lag(score, n =5, order_by = year)) %>%
ungroup()
rho = c()
# for each of the lags, calculate the correlation between current and the lag and store in vector rho
for (lag in seq(1,5)) {
rho <- c(rho, cor(state_lags$score, state_lags[[paste("read_lag",as.character(lag),sep="")]], use = "pairwise.complete.obs"))
}
# Save the values of rho for use in the working paper
temp_df <- tibble(rho = rho)
temp_df$subject <- subject
temp_df$state_grade <- state_grade
temp_df$lag <- 1:5
write_csv(temp_df, file.path(output, "intermediate", paste(subject, "-", state_grade, "- rho.csv")))
# Calculate variance of the transitory component by comparing rho_1 with the decay of rho after that
var_state_levels_persistent2 <- rho[1]/mean(rho/lag(rho), na.rm = TRUE)*var_state_levels
# Calculate the share of variance coming from other transitory sources.
# We will need this to calculate the variance breakup for changes
var_state_levels_other2 <- var_state_levels - var_state_levels_persistent2 - var_state_sampling
# Save a graph showing the decay of the correlation
quick_df <- tibble(lag = seq(0,5), correlation = c(1,rho))
ggplot(quick_df, aes(x= lag, y = correlation)) +
geom_line() +
geom_point() +
ylim(0,1)
# ggsave(paste(subject, "-", state_grade, "- correlation_decay.png"), width = 5, height = 6 , path = output)
# Create empty tibble to store results
# I
df_bar <- tibble(state_or_dist = rep(c("State","District"),each =6),
changes_or_levels =rep(rep(c("Changes","Levels"), each =3), times= 2),
bar_part = factor(rep(c("Persistent","Transitory sampling", "Transitory other"),4), levels = c("Transitory other","Transitory sampling", "Persistent")),
value = rep(.001,12)
)
# Update the values for the district changes bar
df_bar$value[(df_bar$state_or_dist == "District") & (df_bar$changes_or_levels == "Changes") & (df_bar$bar_part == "Persistent")] <- var_dist_deltas - 2*var_dist_epsilon
df_bar$value[(df_bar$state_or_dist == "District") & (df_bar$changes_or_levels == "Changes") & (df_bar$bar_part == "Transitory sampling")] <- 2*var_dist_sampling
df_bar$value[(df_bar$state_or_dist == "District") & (df_bar$changes_or_levels == "Changes") & (df_bar$bar_part == "Transitory other")] <- 2*var_dist_epsilon - 2*var_dist_sampling
# Update the values for the district levels bar
df_bar$value[(df_bar$state_or_dist == "District") & (df_bar$changes_or_levels == "Levels") & (df_bar$bar_part == "Persistent")] <- var_dist_levels - var_dist_epsilon
df_bar$value[(df_bar$state_or_dist == "District") & (df_bar$changes_or_levels == "Levels") & (df_bar$bar_part == "Transitory sampling")] <- var_dist_sampling
df_bar$value[(df_bar$state_or_dist == "District") & (df_bar$changes_or_levels == "Levels") & (df_bar$bar_part == "Transitory other")] <- var_dist_epsilon - var_dist_sampling
### STATE DATA ###
# CHANGES
df_bar$value[(df_bar$state_or_dist == "State") & (df_bar$changes_or_levels == "Changes") & (df_bar$bar_part == "Persistent")] <- var_state_deltas-2*(var_state_sampling+var_state_levels_other2)
df_bar$value[(df_bar$state_or_dist == "State") & (df_bar$changes_or_levels == "Changes") & (df_bar$bar_part == "Transitory sampling")] <- 2*var_state_sampling
df_bar$value[(df_bar$state_or_dist == "State") & (df_bar$changes_or_levels == "Changes") & (df_bar$bar_part == "Transitory other")] <- 2*var_state_levels_other2
# LEVELS
df_bar$value[(df_bar$state_or_dist == "State") & (df_bar$changes_or_levels == "Levels") & (df_bar$bar_part == "Persistent")] <- var_state_levels_persistent2
df_bar$value[(df_bar$state_or_dist == "State") & (df_bar$changes_or_levels == "Levels") & (df_bar$bar_part == "Transitory sampling")] <- var_state_sampling
df_bar$value[(df_bar$state_or_dist == "State") & (df_bar$changes_or_levels == "Levels") & (df_bar$bar_part == "Transitory other")] <- var_state_levels_other2
# I have created this graph so that you can see it in the notebook
# The final version which combines both subjects, is created in a separate notebook
ggplot(df_bar, aes(fill = bar_part, y=value, x= changes_or_levels)) +
geom_bar(position="stack", stat ="identity") +
facet_grid(~ state_or_dist) +
scale_fill_manual(values = c("red", "orange", "blue"))+
labs(fill = "Variance component", x = "")
# ggsave(paste(subject, "-", state_grade," - variance_decomposition.png"), width = 5, height = 6 , path = output)
df_bar$subject <- subject
df_bar$state_grade <- state_grade
write_csv(df_bar, file.path(output, "intermediate", paste(subject, "-", state_grade, "- bar data.csv")))
df_final <- df_bar %>%
pivot_wider(names_from = "bar_part", values_from= "value") %>%
mutate(total_var = `Persistent`+`Transitory sampling`+`Transitory other`) %>%
mutate(share_pers = `Persistent`/total_var, share_samp = `Transitory sampling`/total_var, share_other = `Transitory other`/total_var)
df_final$subject <- subject
df_final$state_grade <- state_grade
write_csv(df_final, file.path(output, "intermediate", paste(subject, "-", state_grade, "- var deco.csv")))
library(tidyverse)
int_path <- "C:/Users/dougj/Dropbox/Education in India/Original research/aservnas/figures/intermediate"
output <- "C:/Users/dougj/Dropbox/Education in India/Original research/aservnas/figures"
file_type <- c("rho", "corr dbl", "var deco", "bar data", "alt ests")
tables <- list()
for (type in file_type) {
tables[[type]] <- tibble(path = list.files(path = int_path, pattern = type)) %>%
rowwise() %>%
summarise(read_csv(file.path(int_path, path)))
}
# Reshape the rho data and then save
tables[['rho']] %>%
pivot_wider(names_from = c("subject", "state_grade"), values_from = rho) %>%
write_csv(file.path(output, "rho.csv"))
tables[["var deco"]] %>%
filter(!((state_or_dist == "District") & (state_grade == 5))) %>%
write_csv(file.path(output, "var deco.csv"))
tables[["alt ests"]]
tables[["var deco"]]
# Fix up the alt ests table so that I can merge it with
alt_ests <- tables[["alt ests"]] %>%
rename(changes_or_levels = changes_levels)
alt_ests
# Fix up the alt ests table so that I can merge it with
alt_ests <- tables[["alt ests"]] %>%
rename(changes_or_levels = changes_levels, state_grade = grade, alt_share_persist, share_persist, alt_share_sampling, share_sampling, alt_share_other = share_other)
# Fix up the alt ests table so that I can merge it with
alt_ests <- tables[["alt ests"]] %>%
rename(changes_or_levels = changes_levels, state_grade = grade, alt_share_persist = share_persist, alt_share_sampling, share_sampling, alt_share_other = share_other)
# Fix up the alt ests table so that I can merge it with
alt_ests <- tables[["alt ests"]] %>%
rename(changes_or_levels = changes_levels, state_grade = grade, alt_share_persist = share_persist, alt_share_sampling = share_sampling, alt_share_other = share_other)
alt_ests
str_to_sentence(alt_ests$changes_or_levels)
# Fix up the alt ests table so that I can merge it with
alt_ests <- tables[["alt ests"]] %>%
rename(changes_or_levels = changes_levels, state_grade = grade, alt_share_persist = share_persist, alt_share_sampling = share_sampling, alt_share_other = share_other) %>%
mutate(str_to_sentence(changes_or_levels))
alt_ests
# Fix up the alt ests table so that I can merge it with
alt_ests <- tables[["alt ests"]] %>%
rename(changes_or_levels = changes_levels, state_grade = grade, alt_share_persist = share_persist, alt_share_sampling = share_sampling, alt_share_other = share_other) %>%
mutate(changes_or_levels = str_to_sentence(changes_or_levels))
alt_ests
tables[["var deco"]]
# Fix up the alt ests table so that I can merge it with
alt_ests <- tables[["alt ests"]] %>%
rename(changes_or_levels = changes_levels, state_grade = grade, alt_share_persist = share_persist, alt_share_sampling = share_sampling, alt_share_other = share_other) %>%
mutate(changes_or_levels = str_to_sentence(changes_or_levels), state_or_dist = "State")
tables[["var deco"]] %>%
filter(!((state_or_dist == "District") & (state_grade == 5))) %>%
left_join(alt_ests)
tables[["var deco"]] %>%
filter(!((state_or_dist == "District") & (state_grade == 5))) %>%
left_join(alt_ests) %>%
write_csv(file.path(output, "var deco.csv"))
tables[["var deco"]] %>%
filter(!((state_or_dist == "District") & (state_grade == 5))) %>%
left_join(alt_ests) %>%
write_csv(file.path(output, "var deco.csv"))
library(tidyverse)
int_path <- "C:/Users/dougj/Dropbox/Education in India/Original research/aservnas/figures/intermediate"
output <- "C:/Users/dougj/Dropbox/Education in India/Original research/aservnas/figures"
corr_decay <- tables[["corr dbl"]]
library(tidyverse)
int_path <- "C:/Users/dougj/Dropbox/Education in India/Original research/aservnas/figures/intermediate"
output <- "C:/Users/dougj/Dropbox/Education in India/Original research/aservnas/figures"
file_type <- c("rho", "corr dbl", "var deco", "bar data", "alt ests")
tables <- list()
for (type in file_type) {
tables[[type]] <- tibble(path = list.files(path = int_path, pattern = type)) %>%
rowwise() %>%
summarise(read_csv(file.path(int_path, path)))
}
# Reshape the rho data and then save
tables[['rho']] %>%
pivot_wider(names_from = c("subject", "state_grade"), values_from = rho) %>%
write_csv(file.path(output, "rho.csv"))
# Fix up the alt ests table so that I can merge it with
alt_ests <- tables[["alt ests"]] %>%
rename(changes_or_levels = changes_levels, state_grade = grade, alt_share_persist = share_persist, alt_share_sampling = share_sampling, alt_share_other = share_other) %>%
mutate(changes_or_levels = str_to_sentence(changes_or_levels), state_or_dist = "State")
tables[["var deco"]] %>%
filter(!((state_or_dist == "District") & (state_grade == 5))) %>%
left_join(alt_ests) %>%
write_csv(file.path(output, "var deco.csv"))
corr_decay <- tables[["corr dbl"]]
View(corr_decay)
rho <- tables[["rho"]]
View(rho)
?add_row
rho <- tables[["rho"]] %>%
mutate(sub_grade = paste(subject, state_grade)) %>%
select(subject, state_grade) %>%
add_row(sub_grade = "math 3", lag = 0, rho = 1)
rho <- tables[["rho"]] %>%
mutate(sub_grade = paste(subject, state_grade)) %>%
select(-subject, -state_grade) %>%
add_row(sub_grade = "math 3", lag = 0, rho = 1)
View(rho)
rho <- tables[["rho"]] %>%
mutate(sub_grade = paste(subject, state_grade)) %>%
select(-subject, -state_grade) %>%
add_row(sub_grade = "math 3", lag = 0, rho = 1) %>%
add_row(sub_grade = "math 5", lag = 0, rho = 1) %>%
add_row(sub_grade = "reading 3", lag = 0, rho = 1) %>%
add_row(sub_grade = "reading 5", lag = 0, rho = 1)
ggplot(rho, aes(x = lag, y = rho, group = sub_grade)) %>% geom_line()
ggplot(rho, aes(x = lag, y = rho, group = sub_grade)) +
geom_line()
ggplot(rho, aes(x = lag, y = rho, fill = sub_grade)) +
geom_line()
ggplot(rho, aes(x = lag, y = rho, colour = sub_grade)) +
geom_line()
ggplot(rho, aes(x = lag, y = rho, colour = sub_grade)) +
geom_line() +
scale_colour_manual("Subject and grade") +
labs(y = "correlation", title = "Decay in autocorrelation by lag")
ggplot(rho, aes(x = lag, y = rho, colour = sub_grade)) +
geom_line() +
scale_colour_manual(title = "Subject and grade") +
labs(y = "correlation", title = "Decay in autocorrelation by lag")
ggplot(rho, aes(x = lag, y = rho, colour = sub_grade)) +
geom_line() +
scale_colour_manual(name = "Subject and grade") +
labs(y = "correlation", title = "Decay in autocorrelation by lag")
ggplot(rho, aes(x = lag, y = rho, colour = sub_grade)) +
geom_line() +
scale_colour_discrete(name = "Subject and grade") +
labs(y = "correlation", title = "Decay in autocorrelation by lag")
ggsave("Correlation decay.png", path = output)
ggsave("Correlation decay.png", width = 5, height = 3 , path = output)
ggsave("Correlation decay.png", width = 6, height = 3 , path = output)
?scale_colour_discrete
library(tidyverse); library(haven); library(survey); library(ggrepel); library(icc)
library(tidyverse); library(haven); library(survey); library(ggrepel); library(icc)
install.packages("icc")
library(tidyverse); library(haven); library(survey); library(ggrepel); library(ICC)
install.packages(ICC)
install.packages("ICC")
library(tidyverse); library(haven); library(survey); library(ggrepel); library(ICC)
library(tidyverse); library(haven); library(survey); library(ggrepel); library(ICC)
ihds_ind_dir <- "C:/Users/dougj/Documents/Data/IHDS/IHDS 2012/DS0001"
figures <- "C:/Users/dougj/Dropbox/Education in India/Original research/aservnas/figures"
# import ASER and NAS data
aser_nas <- read_csv("https://raw.githubusercontent.com/dougj892/public-datasets/master/ASER%202018%20and%20NAS%202017%20govt%20school%20grade%203%20reading.csv")
# Create graph
ggplot(data = aser_nas,aes(x=aser_rank,y=nas_rank,label= state_abbr)) +
geom_abline(intercept = 0, slope = 1, color="orange")  +
geom_point(color="darkblue")  +
theme_bw() +
labs(title="State Rankings Based on Language Results for Std III Students (Rural)", x = "Rank in ASER (2018)", y = "Rank in NAS (2017)") +
scale_y_continuous(breaks=c(1:28)) + scale_x_continuous(breaks=c(1:28))  + theme(panel.grid.minor = element_blank(), panel.grid.major = element_line(colour = "gray",size=0.1)) +
geom_label_repel()
# Save graph
ggsave("aser_nas_lang_ranking.png", width = 9, height = 6 , path = figures)
# Import NSDP data
nsdp <- read_csv("https://raw.githubusercontent.com/dougj892/public-datasets/master/state%20GDP%20per%20capita.csv")
# Merge NSDP data with ASER and NAS data
aser_nas_nsdp <- aser_nas %>% left_join(nsdp, by = "State")
# Check that there are no states which didn't match
print(paste("Number of unmatched states", sum(is.na(aser_nas_nsdp$`NSDP per capita`))))
# Create a ranking for NSDP
aser_nas_nsdp$gdp_rank <- NA
aser_nas_nsdp$gdp_rank[order(aser_nas_nsdp$`NSDP per capita`, decreasing = TRUE)] <- 1:nrow(aser_nas_nsdp)
# Calculate pairwise correlations for the 3 datasets
temp <- aser_nas_nsdp %>% select(ASER_2018, 'NSDP per capita', NAS)
print("Pearson (regular) correlation:")
cor(temp, method = "pearson", use = "pairwise.complete.obs")
print("Spearman (rank) correlation")
cor(temp, method = "spearman", use = "pairwise.complete.obs")
# Calculate correlations with p-values for null rho = 0
cor.test(temp$ASER_2018,temp$`NSDP per capita`, use = "pairwise.complete.obs", method = "pearson")
cor.test(temp$ASER_2018,temp$`NSDP per capita`, use = "pairwise.complete.obs", method = "spearman")
cor.test(temp$NAS,temp$`NSDP per capita`, use = "pairwise.complete.obs", method = "pearson")
cor.test(temp$NAS,temp$`NSDP per capita`, use = "pairwise.complete.obs", method = "spearman")
aser_trends <- read_csv("https://raw.githubusercontent.com/dougj892/public-datasets/master/ASER%20trends%20over%20time.csv")
corrs <- aser_trends %>%
group_by(year) %>%
summarise(pearson = cor(std3_read_std1_all, std3_subtract_all, use="pairwise.complete.obs", method="pearson"),
spearman = cor(std3_read_std1_all, std3_subtract_all, use="pairwise.complete.obs", method="spearman"))
corrs %>% summarise_all(mean, na.rm = TRUE)
